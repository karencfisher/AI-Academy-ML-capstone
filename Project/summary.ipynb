{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rollup of results from the LSTM and RTP experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect LSTM metrics\n",
    "metrics = []\n",
    "folders = os.listdir('./LSTM/results')\n",
    "for folder in folders:\n",
    "    json_path = os.path.join('./LSTM/results', folder, 'metrics.json')\n",
    "    with open(json_path, 'r') as FP:\n",
    "        m = json.load(FP)\n",
    "    m['result set'] = folder\n",
    "    metrics.append(m)\n",
    "df_lstm = pd.DataFrame(metrics, columns=['result set', 'accuracy', 'precision', \n",
    "                                    'recall', 'f1', 'auc'])\n",
    "df_lstm.sort_values(by='f1', ascending=False, inplace=True)\n",
    "\n",
    "# collect RTP metrics\n",
    "rtp_results_path = os.path.join('./RTP/results', 'test_results.csv')\n",
    "df_rtp = pd.read_csv(rtp_results_path)\n",
    "df_rtp = df_rtp[df_rtp['train/test'] == 'test']\n",
    "df_rtp.drop(columns=['train/test'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall\n",
    "\n",
    "Initially the original positive (shock) and negative (non-shock) datasets were split into training and test sets. The method was to randomly select visits (by the 'VisitIdentifier' column), and pull those out as test examples. The remaining visits were stored as training examples. Those data sets were then stored to be used in all experiments, both with RTP and LSTM models, so as to maintain consistency. It was determined that there were not overlaps between the training and testing sets as well, to avoid data leakage. The test data sets were set to 10% of the total data sets.\n",
    "\n",
    "This way it was possible to properly evaluate the resultant models (within the limits of the available data) with \"unseen\" data, which ideally can give a better idea of how the models may be used in clinical practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RTP results\n",
    "\n",
    "Experiments were run with both Support Vector Machine and Logisitic Regression classifiers. Each was run with a set of parameters for the RTP mining, as follows.\n",
    "\n",
    "1. Maximum gap: explore between 4 and 10 hours, in increments of 1;\n",
    "2. Support for the positive cases (shock): explore between 0.1 and 0.3, in increments of 0.05\n",
    "3. Support for negative cases(non-shock): explore between 0.1 and 0.3, in increments of 0.05\n",
    "\n",
    "In additions, the SVM model was trained with different kernels: ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’.\n",
    "\n",
    "The best results for each classifier were then selected, and the corresponding models retrained and used to predict the held out test data set. These are as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>kernel</th>\n",
       "      <th>max_gap</th>\n",
       "      <th>min_support_pos</th>\n",
       "      <th>min_support_neg</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC(gamma=0.1, kernel='poly')</td>\n",
       "      <td>poly</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.837838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.839506</td>\n",
       "      <td>0.824324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model kernel  max_gap  min_support_pos  \\\n",
       "1  SVC(gamma=0.1, kernel='poly')   poly      9.0              0.1   \n",
       "3           LogisticRegression()    NaN     10.0              0.3   \n",
       "\n",
       "   min_support_neg  accuracy  precision    recall        f1       auc  \n",
       "1              0.1  0.837838   0.755102  1.000000  0.860465  0.837838  \n",
       "3              0.3  0.824324   0.772727  0.918919  0.839506  0.824324  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rtp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model here seems to by the Support Vector Machine, using the 'poly' kernel. On the test data, it showed a recall (or sensitivity) of 100%. That means 100% true positives (and no false negatives). It does show a lower specificity: 67.6%. In other words, a false positive rate of 32.4%. It *is* appropriate to err on the side of false positives, as it may still be worth for a patient to receive closer observation in the next 24h hours. However, too high a false positive rate may be draining of recources unnecessarily as well. One possibility to use a threshold of the probability of shock in the prediction window, rather than a binary prediction, for triage purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were several experiments conducted with LSTM and bidirectional LSTM models. The data was preprocessed in two different ways: time unaware sequences (without concern about the varying intervals between observations), and \"time reconstructed\" sequences, in which an attempt was made to reconstitute the observations as having consistent time intervals (30 minutes). In latter preprocessing (or in the latter case feature engineering) the values were both discretized (as integers) into categorical features, or left as is (no scaling applied).\n",
    "\n",
    "In the \"naive\" approach the data set was reshaped into a 3 dimensional array: the shape being (visits, observations, values) in order to prepare it for passing through the LSTM model. Padding was applied to make the second dimsnsion consistent to a maximum number of time steps. \n",
    "\n",
    "The method of reconsitution consisted of first, using logic parallel to that used in the RTP mining, the observations were first transformed in Multivariant State Sequences. From those a consistent timeline of the events was reconstructed. E.g., given an sequence of MSS:\n",
    "\n",
    "[('temperature', N, 0, 55), ('systolicBP', H, 0, 26), ('systolicBP', N, 27, 63), ('temperature', H, 56, 123), ('systolicBP', L, 90, 122)]\n",
    "\n",
    "It would become (concepptually):\n",
    "\n",
    "Legend:\n",
    "\n",
    "| time (min) | temperature | systolicBP |\n",
    "| ---- | ----------- | ---------- |\n",
    "| 0    | N           | H          |\n",
    "|30    | N           | N          |\n",
    "|60    | N           | N          |\n",
    "|90    | H           | L          |\n",
    "|120   | H           | L          |\n",
    "\n",
    "This was sotred as a 3 dimensional array, the visits, time steps, and values. The second dimension was the maximum number of time steps of all the visits, with the array padded with -10 as needed. The Keras masking layer then preceded the LSTM layer in the model architecture to cause the model to not consider the padding values.\n",
    "\n",
    "One issue was that in either case was with computational recources. There were a handful (3-6) visits with very long time spans: as much as over 30,000 observations. Complete runs would have taken days, even weeks, which would not be possible in the timeframe of this project. In order to make the experiments more tractable (especially on a CPU w/o GPU), the small minority of visits were dropped as \"temporal outliers.\" Even then the reconstructed timelines were run on Google Colab to take advantage of its GPU support.\n",
    "\n",
    "In each case, the best results were selected and the corresponding model was run to predict on the test data set. \n",
    "\n",
    "Results are as shown below, sorted from best to worst:\n",
    "\n",
    "| result set | description |\n",
    "| ----- | ----------- |\n",
    "| naive_results | Vanilla LSTM, unaware of varying time intervals |\n",
    "| bi_results | Bidirectional LSTM, unaware of varying time intervals |\n",
    "| tr_results | Vanilla LSTM, reconstructed timeline with discretized values |\n",
    "| tr_noencode_results | Vanilla LSTM, reconstituted timeline with continuous values |\n",
    "| tr_bi_results | Bidirectional LSTM, reconstructed timeline with discretized values |\n",
    "| tr_noencode_bi_results | Bidirectional LSTM, reconstituted timeline with continuous values |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result set</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>naive_results</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.891892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bi_results</td>\n",
       "      <td>0.878378</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.878378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tr_noencode_bi_results</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.864865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tr_noencode_results</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.837838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tr_results</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.729730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tr_bi_lstm_results</td>\n",
       "      <td>0.554054</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.554054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               result set  accuracy  precision    recall        f1       auc\n",
       "1           naive_results  0.891892   0.837209  0.972973  0.900000  0.891892\n",
       "0              bi_results  0.878378   0.833333  0.945946  0.886076  0.878378\n",
       "3  tr_noencode_bi_results  0.864865   0.846154  0.891892  0.868421  0.864865\n",
       "4     tr_noencode_results  0.837838   0.857143  0.810811  0.833333  0.837838\n",
       "5              tr_results  0.729730   0.717949  0.756757  0.736842  0.729730\n",
       "2      tr_bi_lstm_results  0.554054   0.529412  0.972973  0.685714  0.554054"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the most naive approach (ignoring uneven intervals between time steps) produces the highest accuracy, F1 score, AUC< etc., despite that such time sequences are usually not optimal for plain LSTM layers. Bidirectional LSTM scored only slightly less well.\n",
    "\n",
    "Also, the unscaled values for features (such as systolicPB) gave better results than discretization of the values along quantile lines. Time reconstructed models perform better on the raw values than on discretized values. The latter with bidirectional LSTM performed utterly worse, with a mere accuracy of 55% (not much better than pure chance)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e94a37844a0b2c58d590d23a1d3c94eb0291991d9599c29c548c65f82166f260"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
